{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421e43cf",
   "metadata": {},
   "source": [
    "This notebook contains code examples for the experiments that I ran. I have not included all of the variations that I did, as that would result in a lot of code duplication. I've just given examples for each phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import numpy as np\n",
    "from gymnasium import Wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708eb235",
   "metadata": {},
   "source": [
    "# Phase 0: State and action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Sample action: {env.action_space.sample()}\")\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Sample observation: {env.observation_space.sample()}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f1161",
   "metadata": {},
   "source": [
    "# Phase 1: Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Agent\n",
    "random_env = gym.make(\"LunarLander-v3\")\n",
    "random_rewards = []\n",
    "\n",
    "for _ in range(100):\n",
    "    obs, info = random_env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Random agent just samples from the action space\n",
    "        action = random_env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = random_env.step(action)\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    random_rewards.append(episode_reward)\n",
    "\n",
    "random_env.close()\n",
    "mean_reward = np.mean(random_rewards)\n",
    "\n",
    "print(f\"Random agent mean reward: {np.mean(random_rewards):.2f} +/- {np.std(random_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3503890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN default hyperparameters, vary total_timesteps\n",
    "train_env = gym.make(\"LunarLander-v3\")\n",
    "model = DQN('MlpPolicy', train_env, verbose=0)\n",
    "model.learn(total_timesteps=100_000)\n",
    "\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "train_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e2e5a",
   "metadata": {},
   "source": [
    "# Phase 2: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary exploration fraction\n",
    "train_env = gym.make(\"LunarLander-v3\")\n",
    "model = DQN('MlpPolicy', train_env, exploration_fraction = 0.5, verbose=0)\n",
    "model.learn(total_timesteps=300_000)\n",
    "\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "train_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary final epsilon\n",
    "train_env = gym.make(\"LunarLander-v3\")\n",
    "model = DQN('MlpPolicy', train_env, \n",
    "            learning_rate = 0.0001, \n",
    "            exploration_fraction = 0.5, \n",
    "            exploration_final_eps = 0.25, \n",
    "            verbose=0)\n",
    "model.learn(total_timesteps=300_000)\n",
    "\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "train_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary learning rate\n",
    "train_env = gym.make(\"LunarLander-v3\")\n",
    "model = DQN(\n",
    "    'MlpPolicy', \n",
    "    train_env, \n",
    "    learning_rate = 0.00001, \n",
    "    exploration_fraction = 0.5, \n",
    "    exploration_final_eps = 0.25, \n",
    "    verbose=0)\n",
    "\n",
    "model.learn(total_timesteps=1_000_000)\n",
    "\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "train_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f3510c",
   "metadata": {},
   "source": [
    "# Phase 3: Reward Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160dc81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandingIncentiveWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, airborne_penalty=0.0, leg_contact_bonus_multiplier=1.0):\n",
    "        \"\"\"\n",
    "        Wrapper to modify LunarLander rewards to encourage landing.\n",
    "        \n",
    "        Args:\n",
    "            env: The base LunarLander environment\n",
    "            airborne_penalty: Additional penalty per timestep while airborne (e.g., 0.1)\n",
    "            leg_contact_bonus_multiplier: Multiply the leg contact reward by this factor (e.g., 2.0)\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.airborne_penalty = airborne_penalty\n",
    "        self.leg_contact_bonus_multiplier = leg_contact_bonus_multiplier\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Take a step in the environment\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # The LunarLander observation is an 8-dimensional vector:\n",
    "        # [x_pos, y_pos, x_vel, y_vel, angle, angular_vel, left_leg_contact, right_leg_contact]\n",
    "        # Indices 6 and 7 are boolean values (1.0 or 0.0) indicating leg contact\n",
    "        left_leg_contact = observation[6]\n",
    "        right_leg_contact = observation[7]\n",
    "        \n",
    "        # Start with the original reward\n",
    "        modified_reward = reward\n",
    "        \n",
    "        # Apply airborne penalty if no legs are touching ground\n",
    "        if not (left_leg_contact or right_leg_contact):\n",
    "            modified_reward -= self.airborne_penalty\n",
    "        \n",
    "        # Amplify the leg contact reward if legs are touching\n",
    "        # The original environment already gives +10 per leg per frame\n",
    "        # So we add additional bonus based on the multiplier\n",
    "        if left_leg_contact:\n",
    "            modified_reward += 10 * (self.leg_contact_bonus_multiplier - 1.0)\n",
    "        if right_leg_contact:\n",
    "            modified_reward += 10 * (self.leg_contact_bonus_multiplier - 1.0)\n",
    "            \n",
    "        return observation, modified_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base environment\n",
    "base_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# Wrap it with custom reward wrapper\n",
    "train_env = LandingIncentiveWrapper(\n",
    "    base_env, \n",
    "    airborne_penalty=0.1,  # Small penalty per frame for hovering\n",
    "    leg_contact_bonus_multiplier=2.0  # Double the leg contact reward\n",
    ")\n",
    "\n",
    "# Train\n",
    "model = DQN('MlpPolicy', train_env, verbose=0,\n",
    "            exploration_fraction=0.5,\n",
    "            exploration_final_eps=0.25)\n",
    "model.learn(total_timesteps=500_000)\n",
    "model.save(\"dqn_lunar_stable6\")\n",
    "\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "train_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ece8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineIncentiveWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, main_engine_penalty_multiplier=1.0, \n",
    "                 side_engine_penalty_multiplier=1.0,):\n",
    "        \"\"\"\n",
    "        Wrapper to modify LunarLander engine penalties to discourage hovering.\n",
    "        \n",
    "        The original LunarLander gives:\n",
    "        - 0.3 penalty per frame when main engine fires\n",
    "        - 0.03 penalty per frame when side engine fires\n",
    "        \n",
    "        Args:\n",
    "            env: The base LunarLander environment\n",
    "            main_engine_penalty_multiplier: Multiply main engine penalty by this (e.g., 3.0 makes it 0.9 per frame)\n",
    "            side_engine_penalty_multiplier: Multiply side engine penalty by this (e.g., 3.0 makes it 0.09 per frame)\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.main_engine_penalty_multiplier = main_engine_penalty_multiplier\n",
    "        self.side_engine_penalty_multiplier = side_engine_penalty_multiplier\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Take a step in the environment\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # The action space in LunarLander is discrete with 4 actions:\n",
    "        # 0: do nothing\n",
    "        # 1: fire left orientation engine\n",
    "        # 2: fire main engine\n",
    "        # 3: fire right orientation engine\n",
    "        \n",
    "        # We need to calculate the additional penalty based on which action was taken\n",
    "        # The original environment already applies penalties of -0.3 for main, -0.03 for side\n",
    "        # We want to add extra penalty to make these more expensive\n",
    "        \n",
    "        additional_penalty = 0.0\n",
    "        \n",
    "        if action == 2:  # Main engine\n",
    "            # Original penalty is -0.3, we want to add extra to multiply it\n",
    "            additional_penalty = -0.3 * (self.main_engine_penalty_multiplier - 1.0)\n",
    "        elif action == 1 or action == 3:  # Side engines\n",
    "            # Original penalty is -0.03, we want to add extra to multiply it\n",
    "            additional_penalty = -0.03 * (self.side_engine_penalty_multiplier - 1.0)\n",
    "        \n",
    "        modified_reward = reward + additional_penalty\n",
    "            \n",
    "        return observation, modified_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base environment\n",
    "base_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# Try making engines 3x more expensive\n",
    "train_env = EngineIncentiveWrapper(\n",
    "    base_env, \n",
    "    main_engine_penalty_multiplier=3.0,\n",
    "    side_engine_penalty_multiplier=3.0\n",
    ")\n",
    "\n",
    "# Train\n",
    "model = DQN('MlpPolicy', train_env, verbose=0,\n",
    "            exploration_fraction=0.5,\n",
    "            exploration_final_eps=0.25)\n",
    "model.learn(total_timesteps=500_000)\n",
    "\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "train_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLandingWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, \n",
    "                 airborne_penalty=0.0, \n",
    "                 leg_contact_bonus_multiplier=1.0,\n",
    "                 main_engine_penalty_multiplier=1.0,\n",
    "                 side_engine_penalty_multiplier=1.0):\n",
    "        \"\"\"\n",
    "        Combined wrapper to modify LunarLander rewards to encourage landing.\n",
    "        \n",
    "        This wrapper applies multiple reward modifications:\n",
    "        1. Airborne penalty: small cost per frame when no legs touch ground\n",
    "        2. Leg contact bonus: amplifies the reward for having legs on ground\n",
    "        3. Engine penalties: increases the cost of firing engines\n",
    "        \n",
    "        Args:\n",
    "            env: The base LunarLander environment\n",
    "            airborne_penalty: Additional penalty per timestep while airborne (e.g., 0.1)\n",
    "            leg_contact_bonus_multiplier: Multiply leg contact reward by this (e.g., 2.0)\n",
    "            main_engine_penalty_multiplier: Multiply main engine penalty by this (e.g., 3.0)\n",
    "            side_engine_penalty_multiplier: Multiply side engine penalty by this (e.g., 3.0)\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.airborne_penalty = airborne_penalty\n",
    "        self.leg_contact_bonus_multiplier = leg_contact_bonus_multiplier\n",
    "        self.main_engine_penalty_multiplier = main_engine_penalty_multiplier\n",
    "        self.side_engine_penalty_multiplier = side_engine_penalty_multiplier\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Take a step in the environment\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Extract leg contact information from observation\n",
    "        # Observation: [x_pos, y_pos, x_vel, y_vel, angle, angular_vel, left_leg, right_leg]\n",
    "        left_leg_contact = observation[6]\n",
    "        right_leg_contact = observation[7]\n",
    "\n",
    "        # Start with the original reward from the environment\n",
    "        modified_reward = reward\n",
    "        \n",
    "        # If neither leg is touching, agent pays a small penalty per frame\n",
    "        # This makes hovering accumulate costs over time\n",
    "        if not (left_leg_contact or right_leg_contact):\n",
    "            modified_reward -= self.airborne_penalty\n",
    "        \n",
    "        # Original environment gives +10 per leg per frame\n",
    "        # We add extra bonus based on the multiplier\n",
    "        # This makes the landed state more valuable\n",
    "        if left_leg_contact:\n",
    "            modified_reward += 10 * (self.leg_contact_bonus_multiplier - 1.0)\n",
    "        if right_leg_contact:\n",
    "            modified_reward += 10 * (self.leg_contact_bonus_multiplier - 1.0)\n",
    "        \n",
    "        # Original penalties: -0.3 for main engine, -0.03 for side engines\n",
    "        # We add additional penalties to make hovering more expensive\n",
    "        # Action space: 0=nothing, 1=left engine, 2=main engine, 3=right engine\n",
    "        if action == 2:  # Main engine fired\n",
    "            additional_penalty = -0.3 * (self.main_engine_penalty_multiplier - 1.0)\n",
    "            modified_reward += additional_penalty\n",
    "        elif action == 1 or action == 3:  # Side engine fired\n",
    "            additional_penalty = -0.03 * (self.side_engine_penalty_multiplier - 1.0)\n",
    "            modified_reward += additional_penalty\n",
    "            \n",
    "        return observation, modified_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base environment\n",
    "base_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# Wrap\n",
    "train_env = CombinedLandingWrapper(\n",
    "    base_env,\n",
    "    airborne_penalty=0.1,               # Small time pressure\n",
    "    leg_contact_bonus_multiplier=2.0,   # Double the landing reward\n",
    "    main_engine_penalty_multiplier=3.0, # Triple engine costs\n",
    "    side_engine_penalty_multiplier=3.0\n",
    ")\n",
    "\n",
    "# Train \n",
    "model = DQN('MlpPolicy', train_env, verbose=0,\n",
    "            exploration_fraction=0.5,\n",
    "            exploration_final_eps=0.25)\n",
    "model.learn(total_timesteps=300_000)\n",
    "\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "train_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec636ba",
   "metadata": {},
   "source": [
    "# Phase 5: Research-based Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and evaluation environments\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# Network architecture: [256, 128] hidden layers\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[256, 128]\n",
    ")\n",
    "\n",
    "# Create the DQN agent with paper's hyperparameters\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=0.0005,              \n",
    "    buffer_size=65536,                 \n",
    "    batch_size=32,                     \n",
    "    gamma=0.99,                        \n",
    "    exploration_fraction=0.75,         \n",
    "    exploration_initial_eps=0.5,       \n",
    "    exploration_final_eps=0.0,         \n",
    "    target_update_interval=1000,       \n",
    "    learning_starts=1000,              \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train for 2000 episodes (paper used 2000)\n",
    "# Each episode is roughly 200 steps, so ~400,000 timesteps\n",
    "total_timesteps = 400000\n",
    "\n",
    "print(\"Starting training...\")\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Test the trained model\n",
    "print(\"\\nTesting...\")\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True\n",
    ")\n",
    "print(f\"Mean reward over 100 episodes: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb763d58",
   "metadata": {},
   "source": [
    "# Phase 6: Ablation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and evaluation environments\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# Smaller network\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[64, 64]\n",
    ")\n",
    "\n",
    "# Create the DQN agent with paper's hyperparameters\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=0.0005,              \n",
    "    buffer_size=65536,                 \n",
    "    batch_size=32,                     \n",
    "    gamma=0.99,                        \n",
    "    exploration_fraction=0.75,         \n",
    "    exploration_initial_eps=0.5,       \n",
    "    exploration_final_eps=0.0,         \n",
    "    target_update_interval=1000,       \n",
    "    learning_starts=1000,              \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train for 2000 episodes (paper used 2000)\n",
    "# Each episode is roughly 200 steps, so ~400,000 timesteps\n",
    "total_timesteps = 400000\n",
    "\n",
    "print(\"Starting training...\")\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Test the trained model\n",
    "print(\"\\nTesting...\")\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True\n",
    ")\n",
    "print(f\"Mean reward over 100 episodes: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
